{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed16bcf7-37a9-444c-899a-846369c9ec7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "import albumentations as A\n",
    "import cv2\n",
    "from moviepy import VideoFileClip, AudioFileClip\n",
    "import imageio.v3 as iio\n",
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "from ultralytics import YOLO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17c352f-d6fc-4310-a086-03613eda03da",
   "metadata": {},
   "source": [
    "# A lot of functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439d2227-cda9-487b-8051-98d27f6126d8",
   "metadata": {},
   "source": [
    "## Saving video as frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab5d6163-63f9-415f-b808-c54021831f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_video_as_frames(video, video_frames_path, id_length):\n",
    "\n",
    "    if not os.path.exists(video_frames_path):\n",
    "        os.makedirs(video_frames_path)\n",
    "\n",
    "    pbar = tqdm()\n",
    "    \n",
    "    vidcap = cv2.VideoCapture(video)\n",
    "    success,image = vidcap.read()\n",
    "    count = 0\n",
    "    while success:\n",
    "        image_id = '0' * (id_length - len(str(count))) + str(count)\n",
    "        frame_path = osp.join(video_frames_path, 'frame_{}.png'.format(image_id))\n",
    "        cv2.imwrite(frame_path, image)\n",
    "        success,image = vidcap.read()\n",
    "        count += 1\n",
    "        pbar.update(1)\n",
    "\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3347faa-e311-4f25-812e-0090c4ff57cb",
   "metadata": {},
   "source": [
    "## Finding voting ballot box corners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "884a554b-988c-46a3-bfa7-32b20941165d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_in_coordinates(file_path):\n",
    "\n",
    "    coordinates = []\n",
    "    file = open(file_path,'r')\n",
    "    while True:\n",
    "        content=file.readline()\n",
    "        if not content:\n",
    "            break\n",
    "        coordinates = [ int(float(x)) for x in content.strip().split(\" \") ]\n",
    "    file.close()\n",
    "\n",
    "    return coordinates\n",
    "\n",
    "\n",
    "def save_corner_coordinates(coordinates, labels_path, file_name):\n",
    "\n",
    "    file_path = os.path.join(labels_path, file_name)\n",
    "\n",
    "    with open(file_path, \"w\") as file:\n",
    "        file.write(str(coordinates[0][0][0]) + \" \" + str(coordinates[0][0][1]) + \" \" + \n",
    "                   str(coordinates[1][0][0]) + \" \" + str(coordinates[1][0][1]) + \" \" + \n",
    "                   str(coordinates[2][0][0]) + \" \" + str(coordinates[2][0][1]) + \" \" + \n",
    "                   str(coordinates[3][0][0]) + \" \" + str(coordinates[3][0][1]))\n",
    "        file.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d57d2e8-6db5-48fc-abfa-fa57ccdba69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lucas_kanade_video(input_video_frames: str, \n",
    "                       labels_path: str,\n",
    "                       maxCorners: int, \n",
    "                       startFeatures):\n",
    "\n",
    "    if not os.path.exists(labels_path):\n",
    "        os.makedirs(labels_path)\n",
    "\n",
    "    # Find video frames\n",
    "    frames = sorted([file for file in os.listdir(input_video_frames) if file.split(\".\")[1] == \"png\"])\n",
    "\n",
    "    # First frame\n",
    "    frame_path = os.path.join(input_video_frames, frames[0])\n",
    "    old_frame = cv2.imread(frame_path)\n",
    "    frame_width = old_frame.shape[1]\n",
    "    frame_height = old_frame.shape[0]\n",
    "\n",
    "    # Initial corners\n",
    "    old_frame_gs = cv2.cvtColor(old_frame, cv2.COLOR_BGR2GRAY) # Grayscale\n",
    "    mask_linestring = np.zeros_like(old_frame, dtype=np.uint8) # create a mask for optical flow\n",
    "    p0 = startFeatures # initial corners\n",
    "    save_corner_coordinates(p0, labels_path, frames[0].split('.')[0]+str(\".txt\"))\n",
    "\n",
    "    # Going through all the frames\n",
    "    for i in tqdm(range(1, len(frames))):\n",
    "        frame_path = os.path.join(input_video_frames, frames[i])\n",
    "        new_frame = cv2.imread(frame_path)\n",
    "        new_frame_gs = cv2.cvtColor(new_frame, cv2.COLOR_BGR2GRAY)\n",
    "        p1, st, err = cv2.calcOpticalFlowPyrLK(old_frame_gs, new_frame_gs, p0, None, (3,3))\n",
    "        save_corner_coordinates(p1, labels_path, frames[i].split('.')[0]+str(\".txt\"))\n",
    "\n",
    "        # make new frame the old one\n",
    "        old_frame_gs = new_frame_gs.copy()\n",
    "        p0 = p1.reshape(-1,1,2)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb6d7e1-c43e-4952-9856-2dcc6166c44b",
   "metadata": {},
   "source": [
    "## Yolo predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a577df21-4f57-4212-bfc2-89d941b51b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tracking_with_bounding_boxes(model, video_frames_path, video_bboxes_path, tracked_labels_path, add_to_boxes_width, add_to_boxes_height):\n",
    "\n",
    "    first_3_not_found = True\n",
    "    first_3_index = 0\n",
    "\n",
    "    if not os.path.exists(video_bboxes_path):\n",
    "        os.makedirs(video_bboxes_path)\n",
    "\n",
    "    frames = [img for img in os.listdir(video_frames_path) if img.endswith(\".png\")]\n",
    "    frames.sort()\n",
    "\n",
    "    for i in range(len(frames)):\n",
    "        frame_path = osp.join(video_frames_path, frames[i])\n",
    "        frame = cv2.cvtColor(cv2.imread(frame_path), cv2.COLOR_BGR2RGB)\n",
    "        height, width, channels = frame.shape\n",
    "    \n",
    "        results = model.track(frame, \n",
    "                              imgsz = 1920,\n",
    "                              conf = 0.2,\n",
    "                              iou = 0.8,\n",
    "                              persist=True)\n",
    "\n",
    "        classes = np.array(results[0].boxes.cls.cpu())\n",
    "        bboxes = np.array(results[0].boxes.xywhn.cpu())\n",
    "        frame_labels = np.array([np.insert(bboxes[i], 0, classes[i]) for i in range(len(bboxes))])\n",
    "\n",
    "        \n",
    "        # Reading in box corner info\n",
    "        labels_path = os.path.join(tracked_labels_path, frames[i].replace(\"png\", \"txt\"))\n",
    "        corner_labels = read_in_coordinates(labels_path)\n",
    "\n",
    "        x_coord = [corner_labels[j] for j in range(0, len(corner_labels), 2)]\n",
    "        y_coord = [corner_labels[j+1] for j in range(0, len(corner_labels)-1, 2)]\n",
    "        xmin, xmax, ymin, ymax = min(x_coord)/width, max(x_coord)/width, min(y_coord)/height, max(y_coord)/height\n",
    "\n",
    "        \n",
    "        # Going through and filtering YOLO results\n",
    "        filtered_labels = []\n",
    "        for j in range(len(frame_labels)):\n",
    "\n",
    "            bounding_box = frame_labels[j]\n",
    "            if bounding_box[1] > xmin and bounding_box[1] < xmax and bounding_box[2] > ymin and bounding_box[2] < ymax:\n",
    "\n",
    "                if len(filtered_labels) == 0:\n",
    "                    filtered_labels.append(frame_labels[j]) \n",
    "                else:\n",
    "                    new_object = True\n",
    "                    for k in range(j):\n",
    "                        diffrence = abs(frame_labels[j][1] - frame_labels[k][1])\n",
    "                        if diffrence <= 0.001:\n",
    "                            new_object = False\n",
    "                            break\n",
    "                    if new_object: # Only adding this bounding box if it actually is a new digit and not very close prediction to already existing digit\n",
    "                        filtered_labels.append(frame_labels[j])\n",
    "\n",
    "        \n",
    "        if len(filtered_labels) == 3 and first_3_not_found: # Finding the first frame where we have found all 3 digits \n",
    "            first_3_index = i\n",
    "            first_3_not_found = False\n",
    "\n",
    "        found_bboxes = min(len(filtered_labels), 3) # Only saving the first 3 digits\n",
    "\n",
    "        labels_file_name = frames[i].replace(\"frame\", \"bboxes\").replace(\"png\", \"txt\")\n",
    "        labels_path = osp.join(video_bboxes_path, labels_file_name)\n",
    "\n",
    "        add_width = add_to_boxes_width / width\n",
    "        add_height = add_to_boxes_height / height\n",
    "        \n",
    "        # write the label and bounding boxes\n",
    "        if found_bboxes == 0:\n",
    "            with open(labels_path, \"w\") as file:\n",
    "                pass \n",
    "        else:\n",
    "            with open(labels_path, \"w\") as file:\n",
    "                for j in range(found_bboxes):\n",
    "                    file.write(str(int(filtered_labels[j][0])) + \" \" +\n",
    "                           str(float(filtered_labels[j][1])) + \" \" + \n",
    "                           str(float(filtered_labels[j][2])) + \" \" + \n",
    "                           str(float(filtered_labels[j][3]) + add_width ) + \" \" + \n",
    "                           str(float(filtered_labels[j][4]) + add_height) + \"\\n\")\n",
    "                file.close()  \n",
    "\n",
    "\n",
    "    return first_3_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5523e0-74e0-4d64-82b5-8e9e77f512bb",
   "metadata": {},
   "source": [
    "## Reading and saving label info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb60aaed-1e49-46df-be8f-6c656825f307",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reading_in_bbox_txt_file(bbox_path) :\n",
    "    bboxes = []\n",
    "    file = open(bbox_path,'r')\n",
    "    while True:\n",
    "        content=file.readline()\n",
    "        if not content:\n",
    "            break\n",
    "        elements = [ float(x) for x in content.strip().split(\" \") ]\n",
    "        bboxes.append(elements)\n",
    "    file.close()\n",
    "\n",
    "    if len(bboxes) != 0:\n",
    "        sorted_indices = np.argsort(np.array(bboxes)[:, 1]) # Based on xcenter\n",
    "        return np.array(bboxes)[sorted_indices]\n",
    "    else:\n",
    "        return bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7913a8cb-550e-4fe7-a8d6-6e550309c031",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_bboxes_into_txt(path, bboxes):\n",
    "    if len(bboxes) == 0:\n",
    "        with open(path, \"w\") as file:\n",
    "            pass \n",
    "    else:\n",
    "        with open(path, \"w\") as file:\n",
    "            for j in range(len(bboxes)):\n",
    "                if len(bboxes[j]) == 0:\n",
    "                    continue\n",
    "                else:\n",
    "                    file.write(str(int(bboxes[j][0])) + \" \" +\n",
    "                           str(float(bboxes[j][1])) + \" \" + \n",
    "                           str(float(bboxes[j][2])) + \" \" + \n",
    "                           str(float(bboxes[j][3])) + \" \" + \n",
    "                           str(float(bboxes[j][4])) + \"\\n\")\n",
    "            file.close()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4d1821-c68b-4f78-9fa2-ff95e939c164",
   "metadata": {},
   "source": [
    "## Creating frames algorithmically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b04de9f-9da7-4f38-abab-405aa669c0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def creating_missing_bboxes(yolo_boxes_path, bboxes_path, first_3_idx, id_length):\n",
    "\n",
    "    bboxes_files = [f for f in os.listdir(yolo_boxes_path) if f.endswith('.txt')]\n",
    "    bboxes_files.sort()\n",
    "    bboxes_3_labels = []\n",
    "    last_x = [0, 0, 0]\n",
    "    prediction_next = [0, 0, 0]\n",
    "\n",
    "    if not os.path.exists(bboxes_path):\n",
    "        os.makedirs(bboxes_path)\n",
    "\n",
    "\n",
    "    print(\"Creating 3 bbox spaces for every frame\")\n",
    "    for i in tqdm(range(len(bboxes_files))):\n",
    "\n",
    "        frame_label_path = osp.join(yolo_boxes_path, bboxes_files[i])\n",
    "        frame_bboxes = reading_in_bbox_txt_file(frame_label_path)\n",
    "    \n",
    "        if i < first_3_idx: # Before the first 3-digit frame, just save the bounding box info\n",
    "            bboxes_3_labels.append(frame_bboxes)\n",
    "        elif i == first_3_idx: # Save info about the 3-digits\n",
    "            bboxes_3_labels.append(frame_bboxes)\n",
    "            last_x = [frame_bboxes[0][1], frame_bboxes[1][1], frame_bboxes[2][1]]\n",
    "            prediction_next = [frame_bboxes[0][1], frame_bboxes[1][1], frame_bboxes[2][1]]\n",
    "        else:\n",
    "            if len(frame_bboxes) == 0: # Frame empty, but save 3 arrays\n",
    "                bboxes_3_labels.append([[], [], []])\n",
    "            elif len(frame_bboxes) == 3: # All digits found\n",
    "                bboxes_3_labels.append(frame_bboxes)\n",
    "                prediction_next = [frame_bboxes[0][1] + (frame_bboxes[0][1] - last_x[0]), \n",
    "                                   frame_bboxes[1][1] + (frame_bboxes[1][1] - last_x[1]), \n",
    "                                   frame_bboxes[2][1] + (frame_bboxes[2][1] - last_x[2]) ]\n",
    "                last_x = [frame_bboxes[0][1],frame_bboxes[1][1], frame_bboxes[2][1] ]\n",
    "            else: # 1 or 2 digits found\n",
    "                new_bbox = [[], [], []]\n",
    "                prediction = [prediction_next[0] + (prediction_next[0] - last_x[0]),\n",
    "                              prediction_next[1] + (prediction_next[1] - last_x[1]),\n",
    "                              prediction_next[2] + (prediction_next[2] - last_x[2])]\n",
    "                    \n",
    "                for j in range(len(frame_bboxes)): # Find to which digit this bounding box info belongs to\n",
    "                    digit = frame_bboxes[j]\n",
    "                    differences = abs(prediction_next - frame_bboxes[j][1])\n",
    "                    min_index = min(range(len(differences)), key=differences.__getitem__)\n",
    "                    new_bbox[min_index] = digit\n",
    "                    prediction[min_index] = digit[1] + (digit[1] - last_x[min_index])\n",
    "                    last_x[min_index] = digit[1]\n",
    "                prediction_next = prediction\n",
    "                bboxes_3_labels.append(new_bbox)\n",
    "\n",
    "\n",
    "    print(\"Creating missing bboxes\")\n",
    "    for i in range(3): # Going through every digit object\n",
    "        pbar = tqdm(total = len(bboxes_3_labels)-1)\n",
    "\n",
    "        j = first_3_idx\n",
    "        while j < len(bboxes_3_labels)-1:\n",
    "            pbar.update(1)\n",
    "\n",
    "            digit = bboxes_3_labels[j][i] # Starting digit\n",
    "            for k in range(j+1, len(bboxes_3_labels)): # Going through next frames\n",
    "                if len(bboxes_3_labels[k][i]) == 0: # Model didn't find digit from this frame\n",
    "                    if k >= len(bboxes_3_labels)-1:\n",
    "                        j = k\n",
    "                        break\n",
    "                    continue\n",
    "                else: # (some) next frame has this digit\n",
    "                    if j + 1 == k: # We found digit from next frame\n",
    "                        j += 1\n",
    "                        break\n",
    "                    elif k >= len(bboxes_3_labels)-1:\n",
    "                        j = k\n",
    "                        break\n",
    "                    else: # We found digit somewhere further away\n",
    "                        if k-j < 50:\n",
    "                            # We generate bboxes\n",
    "                            generate_frames_nr = k-j-1\n",
    "                            for m in range(1, generate_frames_nr + 1):\n",
    "                                new_frame = [digit[0],\n",
    "                                            digit[1] + ((bboxes_3_labels[k][i][1] - digit[1]) / generate_frames_nr * m),\n",
    "                                            digit[2] + ((bboxes_3_labels[k][i][2] - digit[2]) / generate_frames_nr * m),\n",
    "                                            digit[3] + ((bboxes_3_labels[k][i][3] - digit[3]) / generate_frames_nr * m),\n",
    "                                            digit[4] + ((bboxes_3_labels[k][i][4] - digit[4]) / generate_frames_nr * m)\n",
    "                                            ]\n",
    "                                bboxes_3_labels[j+m][i] = new_frame\n",
    "                        j = k\n",
    "                        break\n",
    "\n",
    "    # Saving new bounding boxes\n",
    "    for i in range(len(bboxes_3_labels)):\n",
    "        image_id = '0' * (id_length - len(str(i))) + str(i)\n",
    "        labels_path = osp.join(bboxes_path, \"bboxes_{}.txt\".format(image_id))\n",
    "        save_bboxes_into_txt(labels_path, bboxes_3_labels[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476e51b7-6d00-4685-81fe-73664d298c04",
   "metadata": {},
   "source": [
    "## Changing the digits in the frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff840c3-4461-4f9a-a1b8-a156dbc19324",
   "metadata": {},
   "source": [
    "### Getting digits from the frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87fe17af-281d-4c65-920b-fea7581ece52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finding_digits_from_the_frame(frame, frame_width, frame_height, frame_labels, padding_up_down, padding_sides):\n",
    "    \n",
    "    digits = []\n",
    "    \n",
    "    for i in range(len(frame_labels)):\n",
    "        digit_width = int(frame_labels[i][3] * frame_width) + padding_sides # Adding more pixels for a boundary\n",
    "        digit_height = int(frame_labels[i][4] * frame_height) + padding_up_down\n",
    "        xmin = int((frame_labels[i][1] * frame_width) - digit_width/2)\n",
    "        ymin = int((frame_labels[i][2] * frame_height) - digit_height/2)\n",
    "    \n",
    "        digit = frame[ymin:ymin+digit_height, xmin:xmin+digit_width]\n",
    "        digits.append(digit)\n",
    "\n",
    "    return digits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98af5c29-576f-4fba-9983-c2cac82cc2c5",
   "metadata": {},
   "source": [
    "### Making the digits into a squares (aka creating more background)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7609686c-79fe-4463-ae23-0e81ac3b67d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def changing_digit_into_square(digits, square_digits_path, frame_id, padding_up_down, padding_sides):\n",
    "\n",
    "    if not os.path.exists(square_digits_path):\n",
    "        os.makedirs(square_digits_path)\n",
    "    if not os.path.exists(osp.join(square_digits_path, \"0\")):\n",
    "        os.makedirs(osp.join(square_digits_path, \"0\"))\n",
    "    if not os.path.exists(osp.join(square_digits_path, \"1\")):\n",
    "        os.makedirs(osp.join(square_digits_path, \"1\"))\n",
    "    if not os.path.exists(osp.join(square_digits_path, \"2\")):\n",
    "        os.makedirs(osp.join(square_digits_path, \"2\"))\n",
    "        \n",
    "    \n",
    "    for i in range(len(digits)):\n",
    "    \n",
    "        digit = digits[i]\n",
    "        digit_width, digit_height = digit.shape[1], digit.shape[0]\n",
    "        square_hw = max(digit_width, digit_height)\n",
    "        square = np.zeros((square_hw, square_hw, 3), dtype=int)\n",
    "    \n",
    "        bigger_height = True if digit_width <= digit_height else False\n",
    "        mask_xmin, mask_ymin, mask_width, mask_height = 0, 0, 0, 0\n",
    "\n",
    "        # When the digit is tall and we have to extend the sides.\n",
    "        if bigger_height:\n",
    "            small_bg = digit[0:digit_height, 0:10]\n",
    "            for j in range(0, square_hw, 10):\n",
    "                if math.floor(square_hw/10)*10 == j: # last part to cover \n",
    "                    new_bg_width = square_hw%10 \n",
    "                    small_bg = digit[0:digit_height, 0:new_bg_width]\n",
    "                    square[0:digit_height, j:j+new_bg_width] = small_bg\n",
    "                else:\n",
    "                    square[0:digit_height, j:j+10] = small_bg\n",
    "            xmin = int((square_hw-digit_width)/2)\n",
    "            square[0:square_hw, xmin:xmin+digit_width] = digit\n",
    "\n",
    "            mask_xmin = int(xmin / square_hw * 255)\n",
    "            mask_ymin = int((padding_up_down/2) / square_hw * 255)\n",
    "            mask_width = int((xmin+digit_width) / square_hw * 255)\n",
    "            mask_height = int((square_hw-(padding_up_down//2)) / square_hw * 255)\n",
    "        \n",
    "        else:  # When the digit is wide and we have to extend the up and down part.\n",
    "            small_bg = digit[0:10, 0:digit_width]\n",
    "            for j in range(0, square_hw, 10):\n",
    "                if math.floor(square_hw/10)*10 == j:\n",
    "                    new_bg_height = square_hw%10 \n",
    "                    small_bg = digit[0:new_bg_height, 0:digit_width]\n",
    "                    square[j:j+new_bg_height, 0:digit_width] = small_bg\n",
    "                else:\n",
    "                    square[j:j+10, 0:digit_width] = small_bg\n",
    "            ymin = int((square_hw-digit_height)/2)\n",
    "            square[ymin:ymin+digit_height, 0:square_hw] = digit\n",
    "\n",
    "            mask_xmin = int((padding_sides//2) / square_hw * 255)\n",
    "            mask_ymin = int(ymin / square_hw * 255)\n",
    "            mask_width = int((square_hw-(padding_sides//2)) / square_hw * 255)\n",
    "            mask_height = int((ymin+digit_height) / square_hw * 255)\n",
    "\n",
    "        # Saving square image\n",
    "        output_image = Image.fromarray(np.uint8(square)).convert('RGB').resize((256,256))\n",
    "        image_path = osp.join(square_digits_path, \"{}/{}_{}.png\".format(i, frame_id, i))\n",
    "        output_image.save(image_path, format='png')\n",
    "\n",
    "        # Saving a mask\n",
    "        mask = np.zeros((256, 256), np.float32)\n",
    "        mask[mask_ymin:mask_height, mask_xmin:mask_width] = 1\n",
    "        mask_path = osp.join(square_digits_path, \"{}/{}_{}_mask.png\".format(i, frame_id, i))\n",
    "        mask = Image.fromarray(np.uint8(mask*255)).convert('L')\n",
    "        mask.save(mask_path, format='png')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6451dfd2-1298-4726-96b5-dbb8ad627117",
   "metadata": {},
   "source": [
    "### WavePaint image generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "713e686a-2dcc-4599-a4d9-808dceb7abd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wavepaint_predictions(square_digits_path, wavepaint_model, wavepaint_predict):\n",
    "\n",
    "    generated_img = osp.join(square_digits_path, \"generated\") \n",
    "    masked_img = osp.join(square_digits_path, \"masked\")\n",
    "\n",
    "    if not os.path.exists(generated_img):\n",
    "        os.makedirs(generated_img)\n",
    "\n",
    "    if not os.path.exists(masked_img):\n",
    "        os.makedirs(masked_img)\n",
    "\n",
    "    # Digit 1\n",
    "    !python {wavepaint_predict} -model_path {wavepaint_model} -test_data {osp.join(square_digits_path, \"0\")} -generated_img {generated_img} -masked_img {masked_img}\n",
    "\n",
    "    # Digit 2\n",
    "    !python {wavepaint_predict} -model_path {wavepaint_model} -test_data {osp.join(square_digits_path, \"1\")} -generated_img {generated_img} -masked_img {masked_img}\n",
    "\n",
    "    # Digit 3\n",
    "    !python {wavepaint_predict} -model_path {wavepaint_model} -test_data {osp.join(square_digits_path, \"2\")} -generated_img {generated_img} -masked_img {masked_img}\n",
    "\n",
    "    results_file_names = [f for f in os.listdir(generated_img) if f.endswith('.png')]\n",
    "    results_file_names.sort()\n",
    "\n",
    "    # Full paths for generated images\n",
    "    predictions = []\n",
    "    for i in range(len(results_file_names)):\n",
    "        if results_file_names[i][-4:] == '.png':\n",
    "            predictions.append(osp.join(generated_img, results_file_names[i]))\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98b2c032-b709-4b6a-b329-aab8f1be0c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generating_new_digits(predictions, frame_labels, frame_width, frame_height, padding_up_down, padding_sides):\n",
    "\n",
    "    new_digits = []\n",
    "    for i in range(len(predictions)):\n",
    "\n",
    "        prediction = cv2.cvtColor(cv2.imread(predictions[i]), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Here we have to find the digit sizes\n",
    "        org_digit_width = int(frame_labels[i][3] * frame_width + padding_sides)\n",
    "        org_digit_height = int(frame_labels[i][4] * frame_height + padding_up_down)\n",
    "\n",
    "        bigger_height = True if org_digit_width <= org_digit_height else False\n",
    "        if bigger_height:\n",
    "            image = cv2.resize(prediction, (org_digit_height, org_digit_height), interpolation=cv2.INTER_CUBIC)\n",
    "            side = int((org_digit_height - org_digit_width)/2)\n",
    "            digit = image[ : , side:side+org_digit_width]\n",
    "        else:\n",
    "            image = cv2.resize(prediction, (org_digit_width, org_digit_width), interpolation=cv2.INTER_CUBIC)\n",
    "            side = int((org_digit_width - org_digit_height)/2)\n",
    "            digit = image[side:side+org_digit_height , :]\n",
    "    \n",
    "        new_digits.append(digit)\n",
    "        i += 1\n",
    "    return new_digits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05ef156-9a1e-40d3-986c-c936deba1ef7",
   "metadata": {},
   "source": [
    "### New digit from an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "abba08c8-d8b0-41fa-9c5c-f29724ff136c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getting_new_digit_image(digit_file, cropp_file):\n",
    "    image = cv2.cvtColor(cv2.imread(digit_file), cv2.COLOR_BGR2RGB)\n",
    "    cropp_coordinates = read_in_coordinates(cropp_file)\n",
    "    cropped = image[cropp_coordinates[2]:cropp_coordinates[3], cropp_coordinates[0]:cropp_coordinates[1]]\n",
    "    blured_digit = bluring(image=np.array(cropped))['image'] # 3, 7, 13\n",
    "    removed_bg = remove_background(cropped)\n",
    "    return removed_bg\n",
    "    \n",
    "\n",
    "def blur_image():\n",
    "    return A.Compose([\n",
    "        A.Blur(p=1, blur_limit=(5, 5))\n",
    "    ])\n",
    "bluring = blur_image()\n",
    "\n",
    "\n",
    "def resize_image(h, w):\n",
    "    return A.Compose([\n",
    "        A.Resize(p=1, height=h, width=w)\n",
    "    ])\n",
    "\n",
    "\n",
    "def remove_background(image):\n",
    "\n",
    "    new_image = np.zeros((image.shape[0], image.shape[1], 4))\n",
    "    \n",
    "    for height in range(len(image)):\n",
    "        for width in range(len(image[height])):\n",
    "            pixel = image[height][width]\n",
    "            if pixel[0] >= 180 and pixel[1] >= 180 and pixel[2] >= 180:\n",
    "                new_image[height][width] = [255, 255, 255, 0]\n",
    "            else:\n",
    "                new_image[height][width] = [pixel[0], pixel[1], pixel[2], 255]\n",
    "                new_image[height][width] = [max(pixel[0]-50, 0), \n",
    "                                            max(pixel[1]-50, 0), \n",
    "                                            max(pixel[2]-50, 0), \n",
    "                                            255]\n",
    "                                            \n",
    "\n",
    "    return new_image\n",
    "\n",
    "\n",
    "def image_perspective_transform(digit, digit_width, digit_height, x_coord, y_coord, xmin, ymin):\n",
    "    \n",
    "    org_corners = np.float32([[0,0],[digit_width, 0], [0, digit_height],[digit_width, digit_height]])\n",
    "    new_corners = np.float32([[x_coord[0] - xmin, y_coord[0] - ymin],\n",
    "                              [x_coord[1] - xmin, y_coord[1] - ymin],\n",
    "                              [x_coord[2] - xmin, y_coord[2] - ymin],\n",
    "                              [x_coord[3] - xmin, y_coord[3] - ymin]])\n",
    "\n",
    "    M = cv2.getPerspectiveTransform(org_corners,new_corners)\n",
    "    transformed = cv2.warpPerspective(digit, M, (digit_width, digit_height),flags=cv2.INTER_LINEAR)\n",
    "\n",
    "    return transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03cba281-5a8e-4fab-bf94-231364b13b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placing new digits into a frame\n",
    "\n",
    "def placing_new_digit_into_frame(frame, new_digit, corner_file_path):\n",
    "    \n",
    "    corners = read_in_coordinates(corner_file_path)\n",
    "\n",
    "    x_coord = []\n",
    "    y_coord = []\n",
    "    for j in range(0, len(corners), 2):\n",
    "        x_coord.append(corners[j])\n",
    "        y_coord.append(corners[j+1])\n",
    "\n",
    "    xmin, xmax = min(x_coord), max(x_coord)\n",
    "    ymin, ymax = min(y_coord), max(y_coord)\n",
    "\n",
    "    digit_width, digit_height = xmax - xmin, ymax - ymin\n",
    "\n",
    "    resizing = resize_image(digit_height, digit_width)\n",
    "    resized_digit = resizing(image=np.array(new_digit))['image']\n",
    "\n",
    "    transformed_digit = image_perspective_transform(resized_digit, digit_width, digit_height, x_coord, y_coord, xmin, ymin)\n",
    "    digit = Image.fromarray(np.uint8(transformed_digit)).convert('RGBA')\n",
    "\n",
    "    # Place new digit into the frame\n",
    "    frame.paste(digit, (xmin, ymin), digit)\n",
    "\n",
    "    return frame\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c381583b-265a-414c-b17a-ee8a76823959",
   "metadata": {},
   "source": [
    "### Placing the new digits back into the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8605ae86-a93b-4559-85cf-d10143c27ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replacing_digits_with_predictions(image_replaced, frame_labels, frame_width, frame_height, generated_digits, changed_frames_path, image_id, padding_up_down, padding_sides):\n",
    "    \n",
    "    for i in range(len(generated_digits)):\n",
    "        # Finding the position\n",
    "        digit_width = int(frame_labels[i][3] * frame_width + padding_sides) # Adding more pixels for a boundary\n",
    "        digit_height = int(frame_labels[i][4] * frame_height + padding_up_down )\n",
    "        xmin = int((frame_labels[i][1] * frame_width) - digit_width/2)\n",
    "        ymin = int((frame_labels[i][2] * frame_height) - digit_height/2)\n",
    "    \n",
    "        # Change digit\n",
    "        digit = (generated_digits[i]).astype('int')\n",
    "        rgba = np.dstack((digit, np.full(digit.shape[:-1], 255)))\n",
    "        digit = Image.fromarray(np.uint8(rgba)).convert('RGBA')\n",
    "    \n",
    "        # Placing digit onto frame\n",
    "        image_replaced.paste(digit, (xmin, ymin), digit)\n",
    "\n",
    "    return image_replaced\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f540768-9b35-4fdb-98cc-336d2b35392a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def changing_digits_in_the_frames(frames_path, \n",
    "                                  bboxes_path, \n",
    "                                  square_digits_path, \n",
    "                                  changed_frames_path, \n",
    "                                  id_length, \n",
    "                                  padding_up_down, \n",
    "                                  padding_sides, \n",
    "                                  wavepaint_model, \n",
    "                                  wavepaint_predict,\n",
    "                                  tracked_labels_path,\n",
    "                                  new_digit):\n",
    "\n",
    "    frames_files = [f for f in os.listdir(frames_path) if f.endswith('.png')]\n",
    "    frames_files.sort()\n",
    "    \n",
    "    bboxes_files = [f for f in os.listdir(bboxes_path) if f.endswith('.txt')]\n",
    "    bboxes_files.sort()\n",
    "\n",
    "    if not os.path.exists(changed_frames_path):\n",
    "        os.makedirs(changed_frames_path)\n",
    "\n",
    "    # Finding frame size\n",
    "    frame_1_path = osp.join(frames_path, frames_files[0])\n",
    "    frame_1 = cv2.cvtColor(cv2.imread(frame_1_path), cv2.COLOR_BGR2RGB)\n",
    "    frame_width = frame_1.shape[1]\n",
    "    frame_height = frame_1.shape[0]\n",
    "\n",
    "    \n",
    "    print(\"Creating square digits\")\n",
    "\n",
    "    for idx in tqdm(range(len(frames_files))):\n",
    "        \n",
    "        frame_path = osp.join(frames_path, frames_files[idx])\n",
    "        frame_np = cv2.cvtColor(cv2.imread(frame_path), cv2.COLOR_BGR2RGB)\n",
    "        label_path = osp.join(bboxes_path, bboxes_files[idx])\n",
    "        frame_labels = reading_in_bbox_txt_file(label_path)\n",
    "        image_id = '0' * (id_length - len(str(idx))) + str(idx)\n",
    "\n",
    "        # Getting the digit from frame\n",
    "        digits = finding_digits_from_the_frame(frame_np, frame_width, frame_height, frame_labels, padding_up_down, padding_sides)\n",
    "\n",
    "        if len(digits) != 0:\n",
    "\n",
    "            # Adding background to the digit\n",
    "            changing_digit_into_square(digits, square_digits_path, image_id, padding_up_down, padding_sides) # Saving all the squared digits\n",
    "\n",
    "    \n",
    "    print(\"WavePaint making predictions\")\n",
    "    \n",
    "    # Generating results\n",
    "    prediction_files = wavepaint_predictions(square_digits_path, wavepaint_model, wavepaint_predict)\n",
    "    prediction_files.sort()\n",
    "\n",
    "    prediction_idx = 0\n",
    "\n",
    "\n",
    "    print(\"Placing predictions and new digit onto frames\")\n",
    "    \n",
    "    # Placing results into a video\n",
    "    for idx in tqdm(range(len(frames_files))):\n",
    "       \n",
    "        frame_path = osp.join(frames_path, frames_files[idx])\n",
    "        frame_image = Image.open(frame_path).convert('RGBA')\n",
    "        label_path = osp.join(bboxes_path, bboxes_files[idx])\n",
    "        frame_labels = reading_in_bbox_txt_file(label_path)\n",
    "        image_id = '0' * (id_length - len(str(idx))) + str(idx)\n",
    "\n",
    "        # Is there a digit to change?\n",
    "        nr_of_digits = len(frame_labels)\n",
    "\n",
    "        if nr_of_digits != 0:\n",
    "            frame_digit_predictions = prediction_files[prediction_idx:prediction_idx+nr_of_digits]\n",
    "\n",
    "            # Formatting the new digits\n",
    "            generated_digits = generating_new_digits(frame_digit_predictions, frame_labels, frame_width, frame_height, padding_up_down, padding_sides)\n",
    "\n",
    "            # Placing digits into the frame and saving it\n",
    "            frame_image = replacing_digits_with_predictions(frame_image, frame_labels, frame_width, frame_height, generated_digits, changed_frames_path, image_id, padding_up_down, padding_sides)\n",
    "            \n",
    "            prediction_idx += nr_of_digits\n",
    "\n",
    "\n",
    "        # Adding new digits onto frame\n",
    "        corner_file_path = os.path.join(tracked_labels_path, frames_files[idx].split('.')[0]+str(\".txt\"))\n",
    "        final_frame = placing_new_digit_into_frame(frame_image, new_digit, corner_file_path)\n",
    "\n",
    "            \n",
    "        # Saving the new frame\n",
    "        file_path = osp.join(changed_frames_path, \"frame_{}_r.png\".format(image_id))\n",
    "        final_frame_resize = final_frame.resize((1280, 720))\n",
    "        final_frame_resize.save(file_path, format='png')\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634d3db2-ce14-4bf5-bd15-37407ac33f03",
   "metadata": {},
   "source": [
    "### Putting the video together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2452d6af-85cf-45eb-adbc-0f2f42223998",
   "metadata": {},
   "outputs": [],
   "source": [
    "def creating_video_from_frames(video_file, frames_folder, video_name):\n",
    "\n",
    "    fps = cv2.VideoCapture(video_file).get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    images = [img for img in os.listdir(frames_folder) if img.endswith(\".png\")]\n",
    "    images.sort(key=lambda x: (int(x.split(\"_\")[-2])))\n",
    "    frame = cv2.imread(os.path.join(frames_folder, images[0]))\n",
    "    height, width, layers = frame.shape\n",
    "    \n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    video = cv2.VideoWriter(video_name, fourcc, fps, (width,height))\n",
    "    \n",
    "    for image in tqdm(images):\n",
    "        video.write(cv2.imread(os.path.join(frames_folder, image)))\n",
    "    video.release()\n",
    "\n",
    "\n",
    "def getting_video_audio(video_file, audio_file):\n",
    "    video_clip = VideoFileClip(video_file)  # Load the video clip\n",
    "    audio_clip = video_clip.audio # Extract the audio from the video clip\n",
    "    audio_clip.write_audiofile(audio_file)  # Write the audio to a separate file\n",
    "    audio_clip.close()\n",
    "    video_clip.close()\n",
    "\n",
    "\n",
    "def adding_audio_to_a_video(video_file, audio_file, video_output):\n",
    "    audio = AudioFileClip(audio_file)\n",
    "    video = VideoFileClip(video_file)\n",
    "    video.audio = audio\n",
    "    video.write_videofile(video_output)\n",
    "\n",
    "\n",
    "\n",
    "def resize_video(input_path, output_path, width, height):\n",
    "\n",
    "    video = VideoFileClip(filename=input_path, target_resolution=(width, height))\n",
    "    video.write_videofile(output_path, fps=30, codec='libx264', audio_codec='aac')\n",
    "    video.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "126f1e98-673e-4d20-9d8c-9142c7ad506d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_files(video_frames, audio_file, video_changed_fps_path, frames_path, yolo_boxes_path, bboxes_path, square_digits_path, changed_frames_path, tracked_labels_path):\n",
    "    \n",
    "    # Remove everything except the final video\n",
    "    os.remove(video_frames)\n",
    "    os.remove(audio_file)\n",
    "    os.remove(video_changed_fps_path)\n",
    "    shutil.rmtree(frames_path, ignore_errors=True)\n",
    "    shutil.rmtree(yolo_boxes_path, ignore_errors=True)\n",
    "    shutil.rmtree(bboxes_path, ignore_errors=True)\n",
    "    shutil.rmtree(square_digits_path, ignore_errors=True)\n",
    "    shutil.rmtree(changed_frames_path, ignore_errors=True)\n",
    "    shutil.rmtree(tracked_labels_path, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f79010-0855-412f-b26d-df447c7b500e",
   "metadata": {},
   "source": [
    "# Start of the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a8b3ace2-4859-4d36-90ff-69f3c9c84fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "video = \"../datasets/voting_ballots_data/videos_filled/videos/per_6_vid_4_filled.mp4\" # original video\n",
    "corner_file_path = \"../datasets/voting_ballots_data/videos_filled/corners/per_6_vid_4_filled.txt\" # first frame corner coordinates\n",
    "new_digit_file_path = \"../datasets/voting_ballots_data/images/images/per_6_img_4.png\" # new digit\n",
    "cropp_file_path = \"../datasets/voting_ballots_data/images/cropped/per_6_img_4.txt\" # new digit cropp\n",
    "save_path = \"pipeline_results\"\n",
    "description = \"final_p6_v4_+_p6_i4\"\n",
    "\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "video_changed_fps_path = \"{}/{}_changed_fps.mp4\".format(save_path, description) # video frames\n",
    "frames_path = \"{}/{}_frames\".format(save_path, description) # video frames\n",
    "yolo_boxes_path = \"{}/{}_yolo\".format(save_path, description)\n",
    "bboxes_path = \"{}/{}_labels\".format(save_path, description)\n",
    "square_digits_path = \"{}/{}_square\".format(save_path, description) # bounding boxes\n",
    "changed_frames_path = \"{}/{}_frames_changed\".format(save_path, description) # video frames\n",
    "tracked_labels_path = \"{}/{}_track_labels\".format(save_path, description) # video frames\n",
    "audio = \"{}/{}_audio.mp3\".format(save_path, description) # audio file\n",
    "video_frames = \"{}/{}_labels.mp4\".format(save_path, description) # video without audio\n",
    "video_final = \"{}/{}.mp4\".format(save_path, description) # video with audio\n",
    "\n",
    "\n",
    "YOLO_model = YOLO(\"../training_YOLO/YOLO11m_e15_img1920_oneClass.pt\")\n",
    "wavepaint_model_file = \"../training_WavePaint/WavePaint_blocks4_dim128_modules6_trained_model.pth\"\n",
    "wavepaint_predict = \"../training_WavePaint/predict.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c333a5d7-7a2a-450e-9642-e32eb8f11999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Change the fps of the video\n",
      "{'video_found': True, 'audio_found': True, 'metadata': {'major_brand': 'isom', 'minor_version': '512', 'compatible_brands': 'isomiso2avc1mp41', 'encoder': 'Lavf58.29.100'}, 'inputs': [{'streams': [{'input_number': 0, 'stream_number': 0, 'stream_type': 'video', 'language': None, 'default': True, 'size': [1920, 1080], 'bitrate': 2742, 'fps': 30.0, 'codec_name': 'h264', 'profile': '(High)', 'metadata': {'Metadata': '', 'handler_name': 'VideoHandler', 'vendor_id': '[0][0][0][0]'}}, {'input_number': 0, 'stream_number': 1, 'stream_type': 'audio', 'language': None, 'default': True, 'fps': 44100, 'bitrate': 127, 'metadata': {'Metadata': '', 'handler_name': 'SoundHandler', 'vendor_id': '[0][0][0][0]'}}], 'input_number': 0}], 'duration': 9.54, 'bitrate': 2869, 'start': 0.0, 'default_video_input_number': 0, 'default_video_stream_number': 0, 'video_codec_name': 'h264', 'video_profile': '(High)', 'video_size': [1920, 1080], 'video_bitrate': 2742, 'video_fps': 30.0, 'default_audio_input_number': 0, 'default_audio_stream_number': 1, 'audio_fps': 44100, 'audio_bitrate': 127, 'video_duration': 9.54, 'video_n_frames': 286}\n",
      "/home/ahabanen/virEnvs/pipeline/lib/python3.11/site-packages/imageio_ffmpeg/binaries/ffmpeg-linux-x86_64-v7.0.2 -i ../datasets/voting_ballots_data/videos_filled/videos/per_6_vid_4_filled.mp4 -loglevel error -f image2pipe -vf scale=1920:1080 -sws_flags bicubic -pix_fmt rgb24 -vcodec rawvideo -\n",
      "MoviePy - Building video pipeline_results/final_p6_v4_+_p6_i4_changed_fps.mp4.\n",
      "MoviePy - Writing audio in final_p6_v4_+_p6_i4_changed_fpsTEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "MoviePy - Writing video pipeline_results/final_p6_v4_+_p6_i4_changed_fps.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready pipeline_results/final_p6_v4_+_p6_i4_changed_fps.mp4\n",
      "Saving video as frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "228it [00:07, 29.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding corners\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 227/227 [00:05<00:00, 41.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding bounding boxes with YOLO\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "HIP out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 63.98 GiB of which 0 bytes is free. Of the allocated memory 756.50 KiB is allocated by PyTorch, and 1.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 22\u001b[0m\n\u001b[1;32m     18\u001b[0m lucas_kanade_video(frames_path, tracked_labels_path, \u001b[38;5;241m4\u001b[39m, corners)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinding bounding boxes with YOLO\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m first_3_idx \u001b[38;5;241m=\u001b[39m \u001b[43mtracking_with_bounding_boxes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mYOLO_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myolo_boxes_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracked_labels_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_to_boxes_width\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_to_boxes_height\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYOLO time \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(point2\u001b[38;5;241m-\u001b[39mpoint1))\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGoing through YOLO predictions\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 17\u001b[0m, in \u001b[0;36mtracking_with_bounding_boxes\u001b[0;34m(model, video_frames_path, video_bboxes_path, tracked_labels_path, add_to_boxes_width, add_to_boxes_height)\u001b[0m\n\u001b[1;32m     14\u001b[0m frame \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(cv2\u001b[38;5;241m.\u001b[39mimread(frame_path), cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[1;32m     15\u001b[0m height, width, channels \u001b[38;5;241m=\u001b[39m frame\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m---> 17\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1920\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m                      \u001b[49m\u001b[43miou\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mpersist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m classes \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(results[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mboxes\u001b[38;5;241m.\u001b[39mcls\u001b[38;5;241m.\u001b[39mcpu())\n\u001b[1;32m     24\u001b[0m bboxes \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(results[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mboxes\u001b[38;5;241m.\u001b[39mxywhn\u001b[38;5;241m.\u001b[39mcpu())\n",
      "File \u001b[0;32m~/virEnvs/pipeline/lib/python3.11/site-packages/ultralytics/engine/model.py:593\u001b[0m, in \u001b[0;36mModel.track\u001b[0;34m(self, source, stream, persist, **kwargs)\u001b[0m\n\u001b[1;32m    591\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# batch-size 1 for tracking in videos\u001b[39;00m\n\u001b[1;32m    592\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrack\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 593\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/virEnvs/pipeline/lib/python3.11/site-packages/ultralytics/engine/model.py:542\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor:\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor \u001b[38;5;241m=\u001b[39m (predictor \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_smart_load(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredictor\u001b[39m\u001b[38;5;124m\"\u001b[39m))(overrides\u001b[38;5;241m=\u001b[39margs, _callbacks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks)\n\u001b[0;32m--> 542\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_cli\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# only update args if predictor is already setup\u001b[39;00m\n\u001b[1;32m    544\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m get_cfg(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39margs, args)\n",
      "File \u001b[0;32m~/virEnvs/pipeline/lib/python3.11/site-packages/ultralytics/engine/predictor.py:384\u001b[0m, in \u001b[0;36mBasePredictor.setup_model\u001b[0;34m(self, model, verbose)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msetup_model\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    377\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;124;03m    Initialize YOLO model with given parameters and set it to evaluation mode.\u001b[39;00m\n\u001b[1;32m    379\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;124;03m        verbose (bool): Whether to print verbose output.\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 384\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mAutoBackend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mselect_device\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdnn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdnn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfp16\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhalf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfuse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    395\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdevice  \u001b[38;5;66;03m# update device\u001b[39;00m\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mhalf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mfp16  \u001b[38;5;66;03m# update half\u001b[39;00m\n",
      "File \u001b[0;32m~/virEnvs/pipeline/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/virEnvs/pipeline/lib/python3.11/site-packages/ultralytics/nn/autobackend.py:162\u001b[0m, in \u001b[0;36mAutoBackend.__init__\u001b[0;34m(self, weights, device, dnn, data, fp16, batch, fuse, verbose)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;66;03m# In-memory PyTorch model\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nn_module:\n\u001b[0;32m--> 162\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mweights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m fuse:\n\u001b[1;32m    164\u001b[0m         model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfuse(verbose\u001b[38;5;241m=\u001b[39mverbose)\n",
      "File \u001b[0;32m~/virEnvs/pipeline/lib/python3.11/site-packages/torch/nn/modules/module.py:1355\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1352\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1353\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1355\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/virEnvs/pipeline/lib/python3.11/site-packages/ultralytics/nn/tasks.py:264\u001b[0m, in \u001b[0;36mBaseModel._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    255\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;124;03m    Apply a function to all tensors in the model that are not parameters or registered buffers.\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;124;03m        (BaseModel): An updated BaseModel object.\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 264\u001b[0m     \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    265\u001b[0m     m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# Detect()\u001b[39;00m\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    267\u001b[0m         m, Detect\n\u001b[1;32m    268\u001b[0m     ):  \u001b[38;5;66;03m# includes all Detect subclasses like Segment, Pose, OBB, WorldDetect, YOLOEDetect, YOLOESegment\u001b[39;00m\n",
      "File \u001b[0;32m~/virEnvs/pipeline/lib/python3.11/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 915\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    919\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    920\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/virEnvs/pipeline/lib/python3.11/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 915\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    919\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    920\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/virEnvs/pipeline/lib/python3.11/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 915\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    919\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    920\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/virEnvs/pipeline/lib/python3.11/site-packages/torch/nn/modules/module.py:942\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    940\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 942\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    943\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    945\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/virEnvs/pipeline/lib/python3.11/site-packages/torch/nn/modules/module.py:1341\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1334\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1335\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1336\u001b[0m             device,\n\u001b[1;32m   1337\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1338\u001b[0m             non_blocking,\n\u001b[1;32m   1339\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1340\u001b[0m         )\n\u001b[0;32m-> 1341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1345\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: HIP out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 63.98 GiB of which 0 bytes is free. Of the allocated memory 756.50 KiB is allocated by PyTorch, and 1.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "id_length = 6\n",
    "padding_up_down = 20 # How many pixels we add beyond bounding box pixels (20 means 10 pixels for each side)\n",
    "padding_sides = 20 \n",
    "add_to_boxes_height = 20 # saving little bit bigger boxes than yolo predicts\n",
    "add_to_boxes_width = 10 # saving little bit bigger boxes than yolo predicts\n",
    "\n",
    "print(\"Change the fps of the video\")\n",
    "video_changed_fps = VideoFileClip(video)\n",
    "video_changed_fps.write_videofile(video_changed_fps_path, fps=24)\n",
    "\n",
    "print(\"Saving video as frames\")\n",
    "save_video_as_frames(video_changed_fps_path, frames_path, id_length)\n",
    "\n",
    "print(\"Finding corners\")\n",
    "coordinates = read_in_coordinates(corner_file_path)\n",
    "corners = np.float32(np.array([[[coordinates[0], coordinates[1]]], [[coordinates[2], coordinates[3]]], [[coordinates[4], coordinates[5]]], [[coordinates[6], coordinates[7]]]]))\n",
    "lucas_kanade_video(frames_path, tracked_labels_path, 4, corners)\n",
    "\n",
    "\n",
    "print(\"Finding bounding boxes with YOLO\")\n",
    "first_3_idx = tracking_with_bounding_boxes(YOLO_model, frames_path, yolo_boxes_path, tracked_labels_path, add_to_boxes_width, add_to_boxes_height)\n",
    "\n",
    "\n",
    "print(\"YOLO time \" + str(point2-point1))\n",
    "\n",
    "print(\"Going through YOLO predictions\")\n",
    "creating_missing_bboxes(yolo_boxes_path, bboxes_path, first_3_idx, id_length)\n",
    "\n",
    "\n",
    "print(\"Getting new digit\")\n",
    "new_digit = getting_new_digit_image(new_digit_file_path, cropp_file_path)\n",
    "\n",
    "\n",
    "print(\"Changing digits in the frames\")\n",
    "changing_digits_in_the_frames(frames_path, \n",
    "                              bboxes_path, \n",
    "                              square_digits_path, \n",
    "                              changed_frames_path, \n",
    "                              id_length, \n",
    "                              padding_up_down, \n",
    "                              padding_sides, \n",
    "                              wavepaint_model_file,\n",
    "                              wavepaint_predict,\n",
    "                              tracked_labels_path,\n",
    "                              new_digit)\n",
    "\n",
    "\n",
    "print(\"Creating video from frames\")\n",
    "creating_video_from_frames(video_changed_fps_path, changed_frames_path, video_frames)\n",
    "\n",
    "print(\"Creating the audio file\")\n",
    "# Creating the audio file\n",
    "getting_video_audio(video_changed_fps_path, audio)\n",
    "\n",
    "print(\"Adding audio to the video\")\n",
    "adding_audio_to_a_video(video_frames, audio, video_final)\n",
    "\n",
    "\n",
    "print(\"Removing unnecessary files\")\n",
    "remove_files(video_frames, audio, video_changed_fps_path, frames_path, yolo_boxes_path, bboxes_path, square_digits_path, changed_frames_path, tracked_labels_path)\n",
    "\n",
    "\n",
    "print(\"Video with changed digits is ready\")\n",
    "\n",
    "end_time = time.time()\n",
    "run_time = end_time - start_time\n",
    "print(\"This video changing took \" + str(run_time) + \" seconds aka \" + str(run_time/60) + \" minutes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdb1255-c758-4c3a-b8bc-19ce01e2d66d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
