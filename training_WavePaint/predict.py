import cv2
import torch
from tqdm import tqdm
import os
from model import WavePaint
from saicinpainting.training.data.datasets import make_default_val_dataset
import argparse 
parser = argparse.ArgumentParser()

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

base_path="/root/ahabanen/WavePaint/WavePaint_"

NUM_MODULES = 6
NUM_BLOCKS = 4 
MODEL_EMBEDDING	= 128 

PATH = base_path + '_blocks'+str(NUM_BLOCKS)+'_dim'+str(MODEL_EMBEDDING)+'_modules'+str(NUM_MODULES)+'_digits9.pth' #path to saved weights

parser.add_argument("-test_data", "--test_data", default="data", help = "Path to images to be tested") # Images with masks
parser.add_argument("-generated_img", "--generated_img", default="data", help = "Path to images generated by the model")
parser.add_argument("-masked_img", "--masked_img", default="data", help = "Path to save masked images")
parser.add_argument("-model_path", "--model_path", default=PATH, help = "Path to model")

args = parser.parse_args()

indir = args.test_data
outdir = args.generated_img
outdir2 = args.masked_img
PATH = args.model_path
out_ext = ".png"


model = WavePaint(
	num_modules		= NUM_MODULES,
	blocks_per_module 	= NUM_BLOCKS,
	mult 			= 2,
	ff_channel 		= MODEL_EMBEDDING,
	final_dim 		= MODEL_EMBEDDING,
	dropout 		= 0.5
).to(device)


#print(PATH)
model.load_state_dict(torch.load(PATH))
print("LOADED GEN WEIGHTS!!!")
model.eval()


dataset = make_default_val_dataset(indir, **{'kind': 'default', 'img_suffix': '.png', 'pad_out_to_modulo': 8})
#print(len(dataset))

for img_i, data in tqdm(enumerate(dataset)):
	mask_fname = dataset.mask_filenames[img_i]
	cur_out_fname = outdir + os.path.splitext(mask_fname[len(indir):])[0] + out_ext

	cur_out_fname2 = outdir2 + os.path.splitext(mask_fname[len(indir):])[0] + out_ext
	os.makedirs(os.path.dirname(cur_out_fname), exist_ok=True)
	os.makedirs(os.path.dirname(cur_out_fname2), exist_ok=True)
	img, mask=torch.Tensor(data["image"]),torch.Tensor(data["mask"])
	h,w=img.shape[2], img.shape[2]
	ground_truth=img.clone().detach()
	img[:, :, :] = img[:, :, :] * (1-mask)
	masked_img=img
	
	out=model.forward((masked_img.reshape(-1,3,h,w)).to(device), mask.reshape(-1,1,h,w).to(device))

	cv2.imwrite(cur_out_fname,cv2.cvtColor(out[0].permute([1,2,0]).cpu().detach().numpy()*255, cv2.COLOR_RGB2BGR))
	cv2.imwrite(cur_out_fname2,cv2.cvtColor(masked_img.permute([1,2,0]).cpu().detach().numpy()*255, cv2.COLOR_RGB2BGR))
